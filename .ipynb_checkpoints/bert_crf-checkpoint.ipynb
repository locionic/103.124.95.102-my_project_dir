{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "590e6d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\t\n",
    "    apr_dir = 'model/'\n",
    "    data_dir = 'corpus/'\n",
    "    model_name = 'model_0.pt'\n",
    "    epoch = 5\n",
    "    bert_model = 'bert-base-cased'\n",
    "    lr = 5e-5\n",
    "    eps = 1e-8\n",
    "    batch_size = 16\n",
    "    mode = 'prediction' # for prediction mode = \"prediction\"\n",
    "    training_data = 'eng.train'\n",
    "    val_data = 'eng.testa'\n",
    "    test_data = 'eng.testa'\n",
    "    test_out = 'test_prediction.csv'\n",
    "    raw_prediction_output = 'raw_prediction.csv'\n",
    "    raw_text = 'mytext.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8806bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a87527d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pickle \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data \n",
    "from transformers import BertTokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "from torchcrf import CRF\n",
    "import timeit\n",
    "import subprocess\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from matplotlib import pyplot as plt \n",
    "import datetime\n",
    "# import Config as config\n",
    "import spacy\n",
    "log_soft = F.log_softmax\n",
    "import sys\n",
    "from optparse import OptionParser\n",
    "\n",
    "#to initialize the network weight with fix seed. \n",
    "def seed_torch(seed=12345):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch()\n",
    "\n",
    "from collections import OrderedDict\n",
    "# read the corpus and return them into list of sentences of list of tokens\n",
    "def corpus_reader(path, delim='\\t', word_idx=0, label_idx=-1):\n",
    "    tokens, labels = [], []\n",
    "    tmp_tok, tmp_lab = [], []\n",
    "    label_set = []\n",
    "    with open(path, 'r') as reader:\n",
    "        for line in reader:\n",
    "            line = line.strip()\n",
    "            cols = line.split(delim)\n",
    "            if len(cols) < 2:\n",
    "                if len(tmp_tok) > 0:\n",
    "                    tokens.append(tmp_tok); labels.append(tmp_lab)\n",
    "                tmp_tok = []\n",
    "                tmp_lab = []\n",
    "            else:\n",
    "                tmp_tok.append(cols[word_idx])\n",
    "                tmp_lab.append(cols[label_idx])\n",
    "                label_set.append(cols[label_idx])\n",
    "    return tokens, labels, list(OrderedDict.fromkeys(label_set))\n",
    "\n",
    "class NER_Dataset(data.Dataset):\n",
    "    def __init__(self, tag2idx, sentences, labels, tokenizer_path = '', do_lower_case=True):\n",
    "        self.tag2idx = tag2idx\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=do_lower_case)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        label = []\n",
    "        for x in self.labels[idx]:\n",
    "            if x in self.tag2idx.keys():\n",
    "                label.append(self.tag2idx[x])\n",
    "            else:\n",
    "                label.append(self.tag2idx['O'])\n",
    "        bert_tokens = []\n",
    "        orig_to_tok_map = []\n",
    "        bert_tokens.append('[CLS]')\n",
    "        #append dummy label 'X' for subtokens\n",
    "        modified_labels = [self.tag2idx['X']]\n",
    "        for i, token in enumerate(sentence):\n",
    "            if len(bert_tokens) >= 512:\n",
    "                break\n",
    "            orig_to_tok_map.append(len(bert_tokens))\n",
    "            modified_labels.append(label[i])\n",
    "            new_token = self.tokenizer.tokenize(token)\n",
    "            bert_tokens.extend(new_token)\n",
    "            modified_labels.extend([self.tag2idx['X']] * (len(new_token) -1))\n",
    "\n",
    "        bert_tokens.append('[SEP]')\n",
    "        modified_labels.append(self.tag2idx['X'])\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "        if len(token_ids) > 511:\n",
    "            token_ids = token_ids[:512]\n",
    "            modified_labels = modified_labels[:512]\n",
    "        return token_ids, len(token_ids), orig_to_tok_map, modified_labels, self.sentences[idx]\n",
    "\n",
    "def pad(batch):\n",
    "    '''Pads to the longest sample'''\n",
    "    get_element = lambda x: [sample[x] for sample in batch]\n",
    "    seq_len = get_element(1)\n",
    "    maxlen = np.array(seq_len).max()\n",
    "    do_pad = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n",
    "    tok_ids = do_pad(0, maxlen)\n",
    "    attn_mask = [[(i>0) for i in ids] for ids in tok_ids] \n",
    "    LT = torch.LongTensor\n",
    "    label = do_pad(3, maxlen)\n",
    "\n",
    "    # sort the index, attn mask and labels on token length\n",
    "    token_ids = get_element(0)\n",
    "    token_ids_len = torch.LongTensor(list(map(len, token_ids)))\n",
    "    _, sorted_idx = token_ids_len.sort(0, descending=True)\n",
    "\n",
    "    tok_ids = LT(tok_ids)[sorted_idx]\n",
    "    attn_mask = LT(attn_mask)[sorted_idx]\n",
    "    labels = LT(label)[sorted_idx]\n",
    "    org_tok_map = get_element(2)\n",
    "    sents = get_element(-1)\n",
    "\n",
    "    return tok_ids, attn_mask, org_tok_map, labels, sents, list(sorted_idx.cpu().numpy())\n",
    "\n",
    "class Bert_CRF(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(Bert_CRF, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "        self.init_weights()\n",
    "        self.crf = CRF(self.num_labels, batch_first=True)    \n",
    "    \n",
    "    def forward(self, input_ids, attn_masks, labels=None):  # dont confuse this with _forward_alg above.\n",
    "        outputs = self.bert(input_ids, attn_masks)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        emission = self.classifier(sequence_output)        \n",
    "        attn_masks = attn_masks.type(torch.uint8)\n",
    "        if labels is not None:\n",
    "            loss = -self.crf(log_soft(emission, 2), labels, mask=attn_masks, reduction='mean')\n",
    "            return loss\n",
    "        else:\n",
    "            prediction = self.crf.decode(emission, mask=attn_masks)\n",
    "            return prediction\n",
    "\n",
    "def generate_training_data(config, bert_tokenizer=\"bert-base\", do_lower_case=True):\n",
    "    training_data, validation_data = config.data_dir+config.training_data, config.data_dir+config.val_data \n",
    "    train_sentences, train_labels, label_set = corpus_reader(training_data, delim=' ')\n",
    "    label_set.append('X')\n",
    "    tag2idx = {t:i for i, t in enumerate(label_set)}\n",
    "    #print('Training datas: ', len(train_sentences))\n",
    "    train_dataset = NER_Dataset(tag2idx, train_sentences, train_labels, tokenizer_path = bert_tokenizer, do_lower_case=do_lower_case)\n",
    "    # save the tag2indx dictionary. Will be used while prediction\n",
    "    with open(config.apr_dir + 'tag2idx.pkl', 'wb') as f:\n",
    "        pickle.dump(tag2idx, f, pickle.HIGHEST_PROTOCOL)\n",
    "    dev_sentences, dev_labels, _ = corpus_reader(validation_data, delim=' ')\n",
    "    dev_dataset = NER_Dataset(tag2idx, dev_sentences, dev_labels, tokenizer_path = bert_tokenizer, do_lower_case=do_lower_case)\n",
    "\n",
    "    #print(len(train_dataset))\n",
    "    train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=config.batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=4,\n",
    "                                collate_fn=pad)\n",
    "    eval_iter = data.DataLoader(dataset=dev_dataset,\n",
    "                                batch_size=config.batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=1,\n",
    "                                collate_fn=pad)\n",
    "    return train_iter, eval_iter, tag2idx\n",
    "\n",
    "def generate_test_data(config, tag2idx, bert_tokenizer=\"bert-base\", do_lower_case=True):\n",
    "    test_data = config.data_dir+config.test_data\n",
    "    test_sentences, test_labels, _ = corpus_reader(test_data, delim=' ')\n",
    "    test_dataset = NER_Dataset(tag2idx, test_sentences, test_labels, tokenizer_path = bert_tokenizer, do_lower_case=do_lower_case)\n",
    "    test_iter = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=config.batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=1,\n",
    "                                collate_fn=pad)\n",
    "    return test_iter\n",
    "\n",
    "def train(train_iter, eval_iter, tag2idx, config, bert_model=\"bert-base-uncased\"):\n",
    "    #print('#Tags: ', len(tag2idx))\n",
    "    unique_labels = list(tag2idx.keys())\n",
    "    model = Bert_CRF.from_pretrained(bert_model, num_labels = len(tag2idx))\n",
    "    model.train()\n",
    "    if torch.cuda.is_available():\n",
    "      model.cuda()\n",
    "    num_epoch = config.epoch\n",
    "    gradient_acc_steps = 1\n",
    "    t_total = len(train_iter) // gradient_acc_steps * num_epoch\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=config.lr, eps=config.eps)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=t_total)\n",
    "    global_step = 0\n",
    "    model.zero_grad()\n",
    "    model.train()\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    train_iterator = trange(num_epoch, desc=\"Epoch\", disable=0)\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    for epoch in (train_iterator):\n",
    "        epoch_iterator = tqdm(train_iter, desc=\"Iteration\", disable=-1)\n",
    "        tr_loss = 0.0\n",
    "        tmp_loss = 0.0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            s = timeit.default_timer()\n",
    "            token_ids, attn_mask, _, labels, _, _= batch\n",
    "            #print(labels)\n",
    "            inputs = {'input_ids' : token_ids.to(device),\n",
    "                     'attn_masks' : attn_mask.to(device),\n",
    "                     'labels' : labels.to(device)\n",
    "                     }  \n",
    "            loss= model(**inputs) \n",
    "            loss.backward()\n",
    "            tmp_loss += loss.item()\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % 1 == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "            if step == 0:\n",
    "                print('\\n%s Step: %d of %d Loss: %f' %(str(datetime.datetime.now()), (step+1), len(epoch_iterator), loss.item()))\n",
    "            if (step+1) % 100 == 0:\n",
    "                print('%s Step: %d of %d Loss: %f' %(str(datetime.datetime.now()), (step+1), len(epoch_iterator), tmp_loss/1000))\n",
    "                tmp_loss = 0.0\n",
    "      \n",
    "        print(\"Training Loss: %f for epoch %d\" %(tr_loss/len(train_iter), epoch))\n",
    "        training_loss.append(tr_loss/len(train_iter))\n",
    "        #'''\n",
    "        #Y_pred = []\n",
    "        #Y_true = []\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        writer = open(config.apr_dir + 'prediction_'+str(epoch)+'.csv', 'w')\n",
    "        for i, batch in enumerate(eval_iter):\n",
    "            token_ids, attn_mask, org_tok_map, labels, original_token, sorted_idx = batch\n",
    "            #attn_mask.dt\n",
    "            inputs = {'input_ids': token_ids.to(device),\n",
    "                      'attn_masks' : attn_mask.to(device)\n",
    "                     }  \n",
    "            \n",
    "            dev_inputs = {'input_ids' : token_ids.to(device),\n",
    "                         'attn_masks' : attn_mask.to(device),\n",
    "                         'labels' : labels.to(device)\n",
    "                         } \n",
    "            with torch.torch.no_grad():\n",
    "                tag_seqs = model(**inputs)\n",
    "                tmp_eval_loss = model(**dev_inputs)\n",
    "            val_loss += tmp_eval_loss.item()\n",
    "            print(\"val loss:\", tmp_eval_loss.item())\n",
    "            #print(labels.numpy())\n",
    "            y_true = list(labels.cpu().numpy())\n",
    "            for i in range(len(sorted_idx)):\n",
    "                o2m = org_tok_map[i]\n",
    "                pos = sorted_idx.index(i)\n",
    "                for j, orig_tok_idx in enumerate(o2m):\n",
    "                    writer.write(original_token[i][j] + '\\t')\n",
    "                    writer.write(unique_labels[y_true[pos][orig_tok_idx]] + '\\t')\n",
    "                    pred_tag = unique_labels[tag_seqs[pos][orig_tok_idx]]\n",
    "                    if pred_tag == 'X':\n",
    "                        pred_tag = 'O'\n",
    "                    writer.write(pred_tag + '\\n')\n",
    "                writer.write('\\n')\n",
    "                \n",
    "        validation_loss.append(val_loss/len(eval_iter))\n",
    "        writer.flush()\n",
    "        print('Epoch: ', epoch)\n",
    "        command = \"python conlleval.py < \" + config.apr_dir + \"prediction_\"+str(epoch)+\".csv\"\n",
    "        process = subprocess.Popen(command,stdout=subprocess.PIPE, shell=True)\n",
    "        result = process.communicate()[0].decode(\"utf-8\")\n",
    "        print(result)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': tr_loss/len(train_iter),\n",
    "        }, config.apr_dir + 'model_' + str(epoch) + '.pt')\n",
    "\n",
    "    total_time = timeit.default_timer() - start_time\n",
    "    print('Total training time: ',   total_time)\n",
    "    return training_loss, validation_loss\n",
    "\n",
    "'''\n",
    "    raw_text should pad data in raw data prediction\n",
    "'''\n",
    "def test(config, test_iter, model, unique_labels, test_output):\n",
    "    model.eval()\n",
    "    writer = open(config.apr_dir + test_output, 'w')\n",
    "    for i, batch in enumerate(test_iter):\n",
    "        token_ids, attn_mask, org_tok_map, labels, original_token, sorted_idx = batch\n",
    "        #attn_mask.dt\n",
    "        inputs = {'input_ids': token_ids.to(device),\n",
    "                  'attn_masks' : attn_mask.to(device)\n",
    "                 }  \n",
    "        with torch.torch.no_grad():\n",
    "            tag_seqs = model(**inputs)\n",
    "        y_true = list(labels.cpu().numpy())\n",
    "        for i in range(len(sorted_idx)):\n",
    "            o2m = org_tok_map[i]\n",
    "            pos = sorted_idx.index(i)\n",
    "            for j, orig_tok_idx in enumerate(o2m):\n",
    "                writer.write(original_token[i][j] + '\\t')\n",
    "                writer.write(unique_labels[y_true[pos][orig_tok_idx]] + '\\t')\n",
    "                pred_tag = unique_labels[tag_seqs[pos][orig_tok_idx]]\n",
    "                if pred_tag == 'X':\n",
    "                    pred_tag = 'O'\n",
    "                writer.write(pred_tag + '\\n')\n",
    "            writer.write('\\n')\n",
    "    writer.flush()\n",
    "    command = \"python conlleval.py < \" + config.apr_dir + test_output\n",
    "    process = subprocess.Popen(command,stdout=subprocess.PIPE, shell=True)\n",
    "    result = process.communicate()[0].decode(\"utf-8\")\n",
    "    print(result)\n",
    "def parse_raw_data(padded_raw_data, model, unique_labels, out_file_name='raw_prediction.csv'):\n",
    "    model.eval()\n",
    "    token_ids, attn_mask, org_tok_map, labels, original_token, sorted_idx = padded_raw_data\n",
    "    #attn_mask.dt\n",
    "    writer = open(out_file_name, 'w')\n",
    "    inputs = {'input_ids': token_ids.to(device),\n",
    "              'attn_masks' : attn_mask.to(device)\n",
    "             }  \n",
    "    with torch.torch.no_grad():\n",
    "        tag_seqs = model(**inputs)\n",
    "    y_true = list(labels.cpu().numpy())\n",
    "    for i in range(len(sorted_idx)):\n",
    "        o2m = org_tok_map[i]\n",
    "        pos = sorted_idx.index(i)\n",
    "        for j, orig_tok_idx in enumerate(o2m):\n",
    "            writer.write(original_token[i][j] + '\\t')\n",
    "            writer.write(unique_labels[y_true[pos][orig_tok_idx]] + '\\t')\n",
    "            pred_tag = unique_labels[tag_seqs[pos][orig_tok_idx]]\n",
    "            if pred_tag == 'X':\n",
    "                pred_tag = 'O'\n",
    "            writer.write(pred_tag + '\\n')\n",
    "        writer.write('\\n')\n",
    "    print(\"Raw data prediction done!\")\n",
    "\n",
    "def show_graph(training_loss, validation_loss, resource_dir):\n",
    "    plt.plot(range(1,len(training_loss)+1), training_loss, label='Training Loss')\n",
    "    plt.plot(range(1,len(training_loss)+1), validation_loss, label='Testing Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(\"Training Loss Vs Testing Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(resource_dir + 'Loss.png')\n",
    "\n",
    "def load_model(config, do_lower_case=True):\n",
    "    f = open(config.apr_dir +'tag2idx.pkl', 'rb')\n",
    "    tag2idx = pickle.load(f)\n",
    "    unique_labels = list(tag2idx.keys())\n",
    "    model = Bert_CRF.from_pretrained(config.bert_model, num_labels=len(tag2idx))\n",
    "    checkpoint = torch.load(config.apr_dir + config.model_name, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    global bert_tokenizer\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(config.bert_model, do_lower_case=do_lower_case)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "    return model, bert_tokenizer, unique_labels, tag2idx\n",
    "\n",
    "def raw_processing(doc, bert_tokenizer, word_tokenizer):\n",
    "    tic = time.time()\n",
    "    spans = re.split(\"[\\n\\r]\", doc)\n",
    "    offset = 0\n",
    "    batch = []\n",
    "    for span in spans:\n",
    "        sentences = sentence_segmenter(span)\n",
    "        for s_idx, sentence in enumerate(sentences.sents):\n",
    "            bert_tokens = []\n",
    "            orig_to_tok_map = []\n",
    "            bert_tokens.append('[CLS]')\n",
    "            begins = []\n",
    "            ends = []\n",
    "            for tok in tokenzer(word):\n",
    "                token = tok.text\n",
    "                offset = doc.find(token, offset)\n",
    "                current_begins.append(offset)\n",
    "                ends.append(offset + len(token))\n",
    "                offset += len(token)\n",
    "                orig_to_tok_map.append(len(bert_tokens))\n",
    "                new_token = bert_tokenizer.tokenize(token)\n",
    "                bert_tokens.extend(new_token)\n",
    "            bert_tokens.append('[SEP]')\n",
    "            token_id = bert_tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "            if len(token_id) > 511:\n",
    "                token_id = token_id[:512]\n",
    "            dummy_labels = ['X'] * len(token_id)\n",
    "            dummy_f_names = ['f_names'] * len(token_id)\n",
    "            sample = (token_id, len(token_id), orig_to_tok_map, dummy_labels, original_token)\n",
    "            batch.append(sample)\n",
    "    pad_data = pad(batch)\n",
    "    return pad_data    \n",
    "\n",
    "def usage(parameter):\n",
    "    parameter.print_help()\n",
    "    print(\"Example usage (training):\\n\", \\\n",
    "        \"\\t python bert_crf.py --mode train \")\n",
    "\n",
    "    print(\"Example usage (testing):\\n\", \\\n",
    "        \"\\t python bert_crf.py --mode test \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
